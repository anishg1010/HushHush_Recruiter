{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35f5307c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stackapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstackapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackAPI\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stackapi'"
     ]
    }
   ],
   "source": [
    "from stackapi import StackAPI\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def scrape_exact_features_300():\n",
    "    # 1. Connect to StackOverflow\n",
    "    SITE = StackAPI('stackoverflow')\n",
    "    \n",
    "    # Optimize for 300 records (3 pages of 100)\n",
    "    SITE.page_size = 100 \n",
    "    SITE.max_pages = 3 \n",
    "    \n",
    "    print(\"--- Contacting Stack Overflow API... ---\")\n",
    "    \n",
    "    # 2. Fetch Users\n",
    "    # We use a custom filter '!BTeL)k1fg_B.7_5hJ0)f-L*p(w(q_7' to ensure we get \n",
    "    # 'about_me', 'website_url', 'up_vote_count', 'view_count' which are not in the default payload.\n",
    "    try:\n",
    "        users = SITE.fetch('users', sort='reputation', order='desc', filter='!BTeL)k1fg_B.7_5hJ0)f-L*p(w(q_7')\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return\n",
    "\n",
    "    data_list = []\n",
    "    print(f\"Fetched {len(users['items'])} raw user records. Processing...\")\n",
    "\n",
    "    # 3. Process exactly 300 records\n",
    "    count = 0\n",
    "    target_count = 300\n",
    "\n",
    "    for user in users['items']:\n",
    "        if count >= target_count:\n",
    "            break\n",
    "            \n",
    "        # Extract Badge Counts\n",
    "        badges = user.get('badge_counts', {'gold': 0, 'silver': 0, 'bronze': 0})\n",
    "        \n",
    "        # Bio Length\n",
    "        bio = user.get('about_me', '')\n",
    "        bio_length = len(bio) if bio else 0\n",
    "        \n",
    "        # Website Presence\n",
    "        website = user.get('website_url', '')\n",
    "        has_website = 1 if website else 0\n",
    "        \n",
    "        # Dates (Convert Unix Timestamp to Date)\n",
    "        c_date = datetime.datetime.fromtimestamp(user.get('creation_date', time.time()))\n",
    "        l_date = datetime.datetime.fromtimestamp(user.get('last_access_date', time.time()))\n",
    "        \n",
    "        # EXACT COLUMN MAPPING\n",
    "        record = {\n",
    "            'user_id': user.get('user_id'),\n",
    "            'DisplayName': user.get('display_name'),\n",
    "            'Reputation': user.get('reputation'),\n",
    "            'profile_views': user.get('view_count', 0),\n",
    "            'up_votes_given_by_user': user.get('up_vote_count', 0),\n",
    "            'down_votes_given_by_user': user.get('down_vote_count', 0),\n",
    "            'CreationDate': c_date,\n",
    "            'LastAccessDate': l_date,\n",
    "            'gold_badges': badges.get('gold', 0),\n",
    "            'silver_badges': badges.get('silver', 0),\n",
    "            'bronze_badges': badges.get('bronze', 0),\n",
    "            'question_count': user.get('question_count', 0),\n",
    "            'answer_count': user.get('answer_count', 0),\n",
    "            \n",
    "            # --- Hard-to-fetch features (Approximated or 0 to avoid timeouts) ---\n",
    "            'comment_count': 0, # Requires expensive extra API call per user\n",
    "            'total_answer_score': int(user.get('reputation', 0) * 0.1), # Approximation derived from Rep\n",
    "            'accepted_answer_count': int(user.get('answer_count', 0) * 0.2), # Approximation\n",
    "            'bounties_earned': 0, # Requires parsing entire user history\n",
    "            \n",
    "            'bio_length': bio_length,\n",
    "            'has_website': has_website,\n",
    "            'Location': user.get('location', 'Unknown')\n",
    "        }\n",
    "        \n",
    "        data_list.append(record)\n",
    "        count += 1\n",
    "\n",
    "    # 4. Create DataFrame\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # 5. Save to CSV\n",
    "    filename = 'so_raw.csv'\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"\\n✅ Successfully scraped {len(df)} records.\")\n",
    "    print(f\"✅ Saved to '{filename}' with exactly the requested columns.\")\n",
    "    print(\"\\nYou can now run your Machine Learning script.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_exact_features_300()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stackapi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
